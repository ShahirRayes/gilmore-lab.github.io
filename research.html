<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title></title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-1.1/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-1.1/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Gilmore Lab</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="research.html">Research</a>
</li>
<li>
  <a href="publications.html">Publications</a>
</li>
<li>
  <a href="parents.html">Parents</a>
</li>
<li>
  <a href="participants.html">Participants</a>
</li>
<li>
  <a href="who-we-are.html">Who we are</a>
</li>
<li>
  <a href="lab-meetings.html">Lab mtgs</a>
</li>
<li>
  <a href="resources.html">Resources</a>
</li>
<li>
  <a href="site-info.html">Site info</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<div id="active-projects" class="section level1">
<h1>Active Projects</h1>
<p><img src="http://databrary.org/theme/img/logo/databrary.png"></p>
<p>The <a href="http://databrary.org">Databrary Project</a> aims to increase scientific transparency and accelerate discovery in developmental science by building the infrastructure for researchers to share video data and related meta-data. The project has five specific aims:</p>
<ul>
<li>Create a web-based <a href="http://databrary.org">data library</a> for sharing and preserving video data and associated meta-data.</li>
<li>Create participant and contributor/user standards that enable open sharing of video data while limiting access to authorized users to ensure participant confidentiality.</li>
<li>Expand the free, open source video coding software, <a href="http://datavyu.org">Datavyu</a> to enable coding, exploring, and analyzing video data.</li>
<li>Build a data management system to support data sharing within labs, among collaborators, and in the Databrary repository.</li>
<li>Transform the culture of developmental science by building a community of researchers committed to open video data sharing.</li>
</ul>
<p>Databrary is an <a href="http://github.com/databrary">open-source</a> software project. Penn State is one of the major “nodes”, with a large number of authorized users.</p>
<p><strong>Publications</strong></p>
<ul>
<li>Gilmore, R. O., Kennedy, J. L., &amp; Adolph, K. E. (2017, September 7). Practical solutions for the ethical sharing of identifiable research data. Retrieved from <a href="http://psyarxiv.com/kew8u" class="uri">http://psyarxiv.com/kew8u</a>.</li>
<li>Gilmore, R.O. &amp; Adolph, K.E. (2017). Video can make behavioural science more reproducible. <em>Nature Human Behaviour</em>. <a href="http://doi.org/10.1038/s41562-017-0128">doi:10.1038/s41562-017-0128</a>.</li>
<li>Gilmore, R.O., Diaz, M.T., Wyble, B.A., &amp; Yarkoni, T. (2017). Progress toward openness, transparency, and reproducibility in cognitive neuroscience. <em>Annals of the New York Academy of Sciences</em>, 1396, 5–18. <a href="http://doi.org/10.1111/nyas.13325">doi: 10.1111/nyas.13325</a>.</li>
<li>Gilmore, R.O., &amp; Adolph, K.E. (2017, February 6). Video can make science more open, transparent, robust, and reproducible. Retrieved from <a href="http://osf.io/3kvp7" class="uri">http://osf.io/3kvp7</a>.</li>
<li>Gilmore, R.O., &amp; Adolph, K.E. (in press). Open sharing of research video: Breaking the boundaries of the research team, in <em>Advancing Social and Behavioral Health Research through Cross-disciplinary Team Science: Principles for Success</em>. Hall, Kara, Croyle, R., &amp; Vogel, A. (Eds.). Springer.</li>
<li>Gilmore, R.O., Adolph, K.E., Millman, D.S. (2016). Curating identifiable data for sharing: The Databrary project. In Proceedings of the 2016 New York Scientific Data Summit. <a href="https://github.com/databrary/presentations/blob/master/nysds-2016/gilmore-adolph-millman-nysds-2016.pdf">PDF of paper</a>.</li>
<li>Gilmore, R.O., Adolph, K.E., Millman, D.S., &amp; Gordon, A. (2016). Transforming education research through open video data sharing. <em>Advances in Engineering Education</em>, <em>5</em>(2). <a href="http://advances.asee.org/publication/transforming-education-research-through-open-video-data-sharing/">HTML</a>.</li>
<li>Gilmore, R.O. (2016). From big data to deep insight in developmental science. <em>Wiley Interdisciplinary Reviews Cognitive Science</em>. <a href="http://doi.org/10.1001/wcs.1379">DOI: 10.1002/wcs.1379</a>.</li>
<li>Gordon, A., Millman, D.S., Steiger, L., Adolph, K.E., &amp; Gilmore, R.O. (2015). Researcher-library collaborations: Data repositories as a service for researchers. <em>Journal of Librarianship and Scholarly Communication</em>. <a href="http://dx.doi.org/10.7710/2162-3309.1238">doi:10.7710/2162-3309.1238</a>.</li>
<li>Adolph, K.E., Gilmore, R.O., Freeman, C., Sanderson, P., &amp; Millman, D. (2012). Toward Open Behavioral Science, <em>Psychological Inquiry: An International Journal for the Advancement of Psychological Theory</em>, <em>23</em>(3), 244-247. <a href="http://dx.doi.org/10.1080/1047840X.2012.705133">doi:10.1080/1047840X.2012.705133</a>.</li>
</ul>
<p><strong>Presentations</strong></p>
<ul>
<li>Gilmore, R.O. (2017, September 29). Data sharing, research ethics, &amp; scientific reproducibility. Talk at the Scholarship and Research Integrity (SARI) workshop series, Penn State. <a href="http://gilmore-lab.github.io/psu-sari-2017-09-28/">HTML slides</a>.</li>
<li>Gilmore, R.O. (2017, September 7). Reproducibility in computationally intensive behavioral research. Talk at the ACI-ICS seminar series, Penn State. <a href="http://gilmore-lab.github.io/aci-ics-2017-09-07/">HTML slides</a>.</li>
<li>Gilmore, R.O. (2017, July 31). Beyond physics envy: Toward a databservatory for human behavior. Lightning talk at the Society for Improving Psychological Science meeting, Charlottesville, VA. <a href="http://gilmore-lab-github.io/sips-2017-databservatory">HTML slides</a>.</li>
<li>Gilmore, R.O. &amp; Nilsonne, G. (2017, July 30). IRBs and the ethical sharing of research data. Talk given at the Society for Improving Psychological Science meeting, Charlottesville, VA. <a href="http://gilmore-lab-github.io/sips-2017-07-30/">HTML slides</a>. <a href="http://osf.io/9d5hr">OSF project</a>.</li>
<li>Gilmore, R.O. (2017, July 26). Yes, we can. Invited panelist at the AERA Workshop on Data Sharing and Research Transparency at the Article Publishing Stage. <a href="http://gilmore-lab.github.io/aera-workshop-2017-07-26/">HTML slides</a>.</li>
<li>Gilmore, R.O. (2017, July 10). The reproducibility crisis in psychology &amp; neuroscience. Talk given at the Penn State Data Reproducibility Bootcamp. <a href="http://gilmore-lab.github.io/psu-data-repro-bootcamp-2017-07-10/">HTML slides</a>.</li>
<li>Adolph, K.E., Binion, G. Gilmore, R.O., Oakes, L., &amp; Vazire, S. (2017, April 6). Openness, replication, &amp; data reuse in developmental science – unique challenges, existing resources, &amp; what is still needed. Invited roundtable at the Society for Research in Child Development meeting, Austin, TX.</li>
<li>Gilmore, R.O. (2017, February 22). A Databservatory for human behavior. Talk given at the Cognitive Area Brown Bag. <a href="https://gilmore-lab.github.io/cog-bbag-talk-2017-02-22">HTML slides</a>.</li>
<li>Gilmore, R.O. (2017, January 31). An -ome of our own: Toward a more reproducible, robust, and insightful science of human behavior. Talk given to the Social Data Analytics (SoDA) 501 students. Penn State University. <a href="http://gilmore-lab.github.io/soda-2017-01-31">HTML slides</a></li>
<li>Gilmore, R.O. (2016, October). The future of big data in developmental science. Talk given at a meeting of the Penn State Child Study Center (CSC) faculty. <a href="http://rawgit.com/gilmore-lab/psu-child-study-ctr-talk-2016-10-28/master/gilmore-csc-talk.html">HTML slides</a>.</li>
<li>Gilmore, R.O. (2016, September). Donald Rumsfeld and the promise of a ‘big data’ science of human behavior. Talk given at a meeting of the Stochastic Modeling and Computational Statistics (SMACS) group, Department of Statistics. <a href="https://rawgit.com/gilmore-lab/psu-stats-smacs-2016-talk/master/gilmore-smacs-2016-09-02.html">HTML slides</a>.</li>
<li>Gilmore, R.O., Adolph, K.E., &amp; Millman, D.S. (2016, August). Curating identifiable data for sharing: The Databrary project. In Proceedings of the 2016 New York Scientific Data Summit. <a href="https://rawgit.com/databrary/presentations/master/nysds-2016/gilmore-nysds-2016.html">HTML slides</a>. <a href="https://github.com/databrary/presentations/blob/master/nysds-2016/gilmore-adolph-millman-nysds-2016.pdf">PDF of paper</a>.</li>
<li>Gilmore, R.O., Adolph, K.E., &amp; Millman, D. (2016, May). Video doesn’t lie: Reproducible workflows with Databrary. Talk given at the NYU Data Science Center <a href="https://reproduciblescience.org/nyu/events/reproducibility-symposium-2016/schedule/">Symposium on Reproducibility</a>. <a href="https://rawgit.com/databrary/presentations/master/nyu-data-science-reproducibility-16/be-bold.html#1">HTML slides</a></li>
<li>Gilmore, R.O., Adolph, K.E., Millman, D.S., Steiger, L., &amp; Simon, D.A. (2015, May). Sharing displays and data from vision science research with Databrary. Poster presented at the Vision Sciences Society meeting, St. Pete Beach, FL. <a href="../pdfs/gilmore-etal-vss-2015.pdf">PDF</a>.</li>
<li>Simon, D.A., Gordon, A.S., Steiger, L., &amp; Gilmore, R.O. (2015, June). Databrary: Enabling sharing and reuse of research video. Proceedings of the 15th ACM/IEEE-CS Joint Conference on Digital Libraries, Knoxville, TN. <a href="https://doi.org/10.1145/2756406.2756951">doi:10.1145/2756406.2756951</a></li>
</ul>
<p><strong>Collaborators</strong></p>
<ul>
<li>Karen Adolph, New York University, Co-Principal Investigator and Project Director</li>
<li>David Millman, New York University, Co-Investigator.</li>
</ul>
<p><strong>Support</strong></p>
<p>This project is supported by the U.S. National Science Foundation (NSF) Grant No. <a href="http://www.nsf.gov/awardsearch/showAward?AWD_ID=1238599">BCS-1238599</a>, the Eunice Kennedy Shriver National Institute of Child Health and Human Development under Cooperative Agreement <a href="http://projectreporter.nih.gov/project_info_description.cfm?aid=8531595&amp;icde=15908155&amp;ddparam=&amp;ddvalue=&amp;ddsub=&amp;cr=1&amp;csb=default&amp;cs=ASC">1-U01-HD-076595-01</a>, the Society for Research in Child Development, the LEGO Foundation, and the Alfred P. Sloan Foundation.</p>
<div id="developmental-dynamics-of-optic-flow-processing" class="section level2">
<h2>Developmental Dynamics of Optic Flow Processing</h2>
<p><img src="images/fesi-2014.jpg" alt="Brain activity" /> <img src="images/optic-flow.jpg" alt="Optic flow" /></p>
<p>Visual motion provides humans and animals with information about their own movement through 3D space and about the structure of the environment – the objects, surfaces, and other animals that it may contain. How the human brain processes complex motion information poses an as-yet unanswered question. This project focuses on characterizing how sensitivity to visual motion emerges in the developing human brain: how brain (EEG) responses to patterns of ego- and object motion emerge, how they develop from infancy through childhood into adulthood, how specific changes in cortical circuitry might account for the observed patterns, and how behavioral sensitivity to motion corresponds to neural activation. The studies compare brain responses and behavioral discrimination patterns in infants, children, and adults to the same types of ego- and object motion. The studies also involve an effort to measure or simulate the statistics of optic flow experienced by infant, child, and adult observers in complex, natural environments using computer vision methods.</p>
<p><strong>Publications</strong></p>
<ul>
<li>Gilmore, R.O., Thomas, A.L., &amp; Fesi, J.D (2016). Children’s brain responses to optic flow vary by pattern type and motion speed. <em>PLoS ONE</em>. <a href="http://doi.org/10.1371/journal.pone.0157911">doi: 10.1371/journal.pone.0157911</a>. Materials on Databrary at <a href="http://doi.org/10.17910/B7QG6W" class="uri">http://doi.org/10.17910/B7QG6W</a>.</li>
<li>Gilmore, R.O., Raudies, F., &amp; Jayaraman, S. (2015). What Accounts for Developmental Shifts in Optic Flow Sensitivity? <em>Proceedings of the IEEE International Conference on Development and Learning and Epigenetic Robotics</em>. <a href="http:/doi.org/10.1109/DEVLRN.2015.7345450">doi:10.1109/DEVLRN.2015.7345450</a>. Materials on Databrary at <a href="http://dx.doi.org/10.17910/B7988V">doi:10.17910/B7988V</a>.</li>
<li>Fesi, J.F., Thomas, A.L., &amp; Gilmore, R.O. (2014). Cortical responses to optic flow and motion contrast across patterns and speeds. <em>Vision Research</em>, 100, 56–71. <a href="http://dx.doi.org/10.1016/j.visres.2014.04.004">doi:10.1016/j.visres.2014.04.004</a>. <a href="https://databrary.org/volume/49">Materials on Databrary</a>.</li>
<li>Raudies, F. &amp; Gilmore, R.O. (2014). Visual motion priors differ for infants and mothers. <em>Neural Computation</em>, <em>26</em>(11), 2652-2668. <a href="http://dx.doi.org/10.1162/NECO_a_00645">doi:10.1162/NECO_a_00645</a>.</li>
<li>Raudies, F., Gilmore, R.O., Kretch, K.S., Franchak, J.M, &amp; Adolph, K.E. (2012). Understanding the development of motion processing by characterizing optic flow experienced by infants and their mothers. <em>Proceedings of the IEEE International Conference on Development and Learning</em>. <a href="http://dx.doi.org/10.1109/DevLrn.2012.6400584">doi:10.1109/DevLrn.2012.6400584</a>.</li>
</ul>
<p><strong>Presentations</strong></p>
<ul>
<li>Gilmore, R.O., Seisler, A., Shade, M., O’Neill, M. (2017, April). School-age children perceive fast radial optic flow in noise more accurately than slow linear flow. Poster presentation at the Society for Research in Child Development, Austin, TX. <a href="https://github.com/gilmore-lab/moco-3-pattern-psychophysics/tree/master/child-motion-psychophysics/pubs/srcd-17-poster/gilmore-seisler-shade-oneill-srcd-2017.pdf">PDF</a>. <a href="http://nyu.databrary.org/volume/218">Databrary</a>. <a href="https://github.com/gilmore-lab/moco-3-pattern-psychophysics/tree/master/child-motion-psychophysics">GitHub</a>.</li>
<li>Gilmore, R.O. (2017, February). Go with the flow: The development of behavioral sensitivity and brain responses to optic flow. Talk at Temple University. <a href="http://gilmore-lab.github.io/temple-2017-02-27">HTML slides</a>. <a href="https://github.com/gilmore-lab/temple-2017-02-27/blob/master/index.md">Markdown</a>.</li>
<li>Gilmore, R.O., Fared, D.A., Dexheimer, M.G., &amp; Seisler, A.R. (2016, November). The appearance and disappearance of visual forms defined by differential motion evokes distinctive EEG responses in school-age children. Presentation at the Society for Neuroscience meeting in San Diego, CA. <a href="https://github.com/gilmore-lab/child-motion-form/blob/master/pubs/sfn-16-poster/poster_landscape.pdf">PDF</a>.</li>
<li>Gilmore, R.O. (2016, October). Go with the flow: The development of behavioral sensitivity and brain responses to optic flow. Talk at the Penn State Action club meeting. <a href="http://rawgit.com/gilmore-lab/psu-action-club-2016-10-28/master/gilmore-action-club.html">HTML slides</a>.</li>
<li>Jayaraman, S., Gilmore, R.O., &amp; Raudies, F. (2016, May). Changes in early optic flow experiences across development and culture. Talk at the International Congress on Infant Studies (ICIS) in New Orleans, LA. <a href="https://rawgit.com/gilmore-lab/ICIS-2016-New-Orleans/master/jayaraman-gilmore-raudies-ICIS-2016.html">HTML slides</a>.</li>
<li>Gilmore, R.O. (2016, September). Open science practices have made my work better. Talk at the Penn State Psychology Cognitive Area brown bag. <a href="https://cdn.rawgit.com/psu-psychology/cognitive/master/brown-bag/2015-09-09-gilmore/cog-bbag-2015-09-09.html">HTML slides</a>.</li>
<li>Adamiak, W., Thomas, A.L., Patel, S.M., &amp; Gilmore. R.O. (2015, May). Adult observers’ sensitivity to optic flow varies by pattern and speed. Poster presented at the Vision Sciences Society meeting, St. Pete’s Beach, FL. <a href="doi:10.1167/15.12.1008" class="uri">doi:10.1167/15.12.1008</a>. <a href="../pdfs/adamiak-etal-vss-2015.pdf">PDF</a>. <a href="http://databrary.org/volume/73">Materials on Databrary</a>.</li>
<li>Raudies, F. &amp; Gilmore, R.O. (2014, May). An analysis of optic flow experienced by infants during natural activities. Poster presented at the Vision Sciences Society meeting, St. Pete Beach, FL. Journal of Vision, 14(10). 226. <a href="doi:10.1167/14.10.226" class="uri">doi:10.1167/14.10.226</a>. <a href="../pdf/raudies-etal-vss-2014.pdf">PDF</a></li>
<li>Thomas, A.L., Fesi, J.D. &amp; Gilmore, R.O. (2014, May). Temporal and speed tuning in brain responses to local and global motion patterns. Poster presented at the Vision Sciences Society meeting, St. Pete Beach, FL. Journal of Vision, 14(10). 482. <a href="doi:10.1167/14.10.482" class="uri">doi:10.1167/14.10.482</a>.</li>
<li>Fesi, J.D., Thomas, A.L., &amp; Gilmore, R.O. (2012, October). Distinct space-time sampling thresholds of VEP responses to optic flow. Poster presented at the Society for Neuroscience meeting, New Orleans, LA. <a href="../pdf/fesi-etal-sfn-2012.pdf">PDF</a></li>
<li>Gilmore, R.O., Raudies, F., Kretch, K.S., Franchak, J.M., &amp; Adolph, K.E. (2012, June). Do you see what I see? Comparing optic flow experienced by infants and their mothers. Poster presented at the International Conference on Infant Studies, Minneapolis, MN. <a href="../pdf/gilmore-etal-icis-2012.pdf">PDF</a>.</li>
<li>Fesi, J.D., Stiffler, J.R., &amp; Gilmore, R.O. (2012, May). Speed tuning of cortical responses to 2D figures defined by motion contrast is non-uniform across contrast types. Poster presented at the Vision Sciences Society meeting, Naples, FL.</li>
<li>Thomas, A.L., Mancino, A.C., Elnathan, H.C., Fesi, J.D., Hwang, K.R., &amp; Gilmore, R.O. (2012, May). Children’s cortical responses to optic flow patterns show differential tuning by pattern type, speed, scalp location, and age group. Poster presented at the Vision Sciences Society meeting, Naples, FL. <a href="../pdf/thomas-etal-vss-2012.pdf">PDF</a>.</li>
<li>Gilmore, R.O., Raudies, F., Kretch, K.S., Franchak, J.M., &amp; Adolph, K.E. (2012, May). Patterns of optic flow experienced by infants and their mothers during locomotion. Poster presented at the Vision Sciences Society meeting, Naples, FL. <a href="../pdf/gilmore-etal-vss-2012.pdf">PDF</a>.</li>
<li>Raudies, F., Kretch, K.S., Franchak, J.M., Mingolla, E., Gilmore, R.O., &amp; Adolph, K.E. (2012, May). Where do mothers point their head when they walk and where do babies point their head when they are carried? Poster presented at the Vision Sciences Society meeting, Naples, FL. <a href="../pdf/raudies-etal-vss-2012.pdf">PDF</a>.</li>
</ul>
<p><strong>Materials</strong></p>
<ul>
<li><a href="http://databrary.org/volume/73" class="uri">http://databrary.org/volume/73</a></li>
<li><a href="http://dx.doi.org/10.17910/B7988V" class="uri">http://dx.doi.org/10.17910/B7988V</a></li>
</ul>
<p><strong>Collaborators</strong></p>
<ul>
<li>Florian Raudies, Hewlett-Packard Research</li>
<li>Swapnaa Jayaraman, Indiana University</li>
<li>Amanda Thomas, Swarthmore College</li>
<li>Jeremy Fesi, U.S. Marine Research</li>
</ul>
<p><strong>Support</strong></p>
<p>This project was supported by the National Science Foundation under grant <a href="http://www.nsf.gov/awardsearch/showAward?AWD_ID=1147440">BCS-1147440</a>.</p>
</div>
<div id="computational-symmetry" class="section level2">
<h2>Computational Symmetry</h2>
<p><img src="images/symmetry-sample-1.png" alt="Symmetric image" /> <img src="images/symmetry-sample-2.png" alt="Symmetric image" /></p>
<p>The ability to sense regular or near-regular patterns serves critical biological needs and is equally important for computer vision and machine intelligence. Despite wide variation in the types of regularity present in natural images, research on human and computer processing of pattern regularity has focused primarily on detecting bilateral reflection symmetry, using largely atheoretical approaches. The goals of this interdisciplinary research are to i) use principles of group theory to develop a conceptual framework for understanding regularity perception and brain activation in humans, and ii) to design general computer-based symmetry detection algorithms that can operate at a level of practical usability.</p>
<p><strong>Presentations</strong></p>
<ul>
<li>Vedak, S.C., Gilmore, R.O., Kohler, P.J., Liu, Y., &amp; Norcia, A.M. (2015, May). The salience of low-order visual features in highly self-similar wallpaper groups. Poster presented at the Vision Sciences Society meeting, St. Pete Beach, FL. <a href="../pdfs/vedak-etal-vss-2015.pdf">PDF</a>. <a href="http://databrary.org/volume/77">Materials on Databrary</a>.</li>
<li>Thomas, A.L., Gilmore, R.O., Norcia, A.M., Liu, Y., Fesi, J.D., Hwang, K.D., Stitt, J., &amp; Liu, J. (2012, October). Visual patterns with rotational symmetry activate distinct cortical regions. Poster presented at the Society for Neuroscience meeting, New Orleans, LA.</li>
</ul>
<p><strong>Collaborators</strong></p>
<ul>
<li>Yanxi Liu, Penn State Computer Science &amp; Engineering</li>
<li>Anthony Norcia, Stanford University, Department of Psychology</li>
</ul>
<p><strong>Support</strong></p>
<p>This project is supported by the National Science Foundation under grant <a href="http://www.nsf.gov/awardsearch/showAward?AWD_ID=1248076">IIS-1248076</a>.</p>
</div>
<div id="the-proximal-emotional-environment-project-peep" class="section level2">
<h2>The Proximal Emotional Environment Project (PEEP)</h2>
<p>A 5-year-old overhears her parents arguing loudly in the next room. She may not understand why they are arguing, but she realizes something is wrong because she perceives anger in their voices. Exposure to interpersonal conflict is consistently associated with less skillful emotion regulation in children although the mechanisms remain to be explained. Because inter-personal conflict is a heterogeneous phenomenon, investigation of the specific features of conflict that contribute to developmental pathways to emotional dysfunction and symptoms requires a process-oriented approach. In this project, we focus on brain responses to angry prosody in natural speech. We are studying young children’s neural processing of angry prosody, spoken by mothers and strangers, as a first step toward a future longitudinal study investigating how the neurocognitive processing of angry prosody mediates relations between conflict exposure in children and the development of anxiety- and anger-related symptoms.</p>
<p><strong>Collaborators</strong></p>
<ul>
<li>Pamela Cole, Penn State</li>
<li>Koraly Perez-Edgar, Penn State</li>
<li>Suzy Scherf, Penn State</li>
<li>Michelle Vigeant, Penn State</li>
</ul>
<p><strong>Support</strong></p>
<p>This project has received support from the Penn State <a href="http://ssri.psu.edu">Social Sciences Research Institute</a> and the National Institute of Mental Health under <a href="http://projectreporter.nih.gov/project_info_description.cfm?aid=8891792&amp;icde=26075719&amp;ddparam=&amp;ddvalue=&amp;ddsub=&amp;cr=2&amp;csb=default&amp;cs=ASC">R21-MH-104547</a>.</p>
<p><strong>Materials</strong></p>
<ul>
<li><a href="https://github.com/gilmore-lab/peep-II">PEEP II repo on GitHub</a>.</li>
<li><a href="https://github.com/gilmore-lab/PEEP-I">PEEP I repo on GitHub</a>.</li>
<li><a href="https://nyu.databrary.org/volume/248">PEEP I stimuli on Databrary</a></li>
</ul>
</div>
<div id="eye-tracking-technologies-to-characterize-and-optimize-visual-attending-in-down-syndrome" class="section level2">
<h2>Eye Tracking Technologies to Characterize and Optimize Visual Attending in Down Syndrome</h2>
<p>Down Syndrome (DS) is the most common known genetic origin of intellectual disability and has an estimated incidence of 1 in every 1000 live births. Such children face unique challenges as they enter into the school years, because the speech that was previously adequate for communication with familiar partners in supportive settings is often not sufficient for academic communication with unfamiliar partners. Indeed, 95% of parents surveyed reported that their children with DS had difficulty being understood by persons outside their immediate social circle (Kumin, 1994). This has significant implications for academic, social, and vocational success; children with limited language skills are at risk of falling behind nondisabled peers academically and experiencing social isolation. Secondary issues often arise when children experience frustration in communication, commonly in the form of challenging behaviors. All aspects of development are further compromised when these behaviors involve aggression toward others, have significant health implications when they are self-injurious, and exacerbate service costs when they necessitate extensive behavior management plans. Children with DS are in desperate need of communication interventions that provide them with the tools to succeed throughout the school years. One form of intervention is called aided augmentative and alternative communication (AAC). In typical clinical applications, aided AAC systems employ picture books, tablet-style computers that present the user with graphic symbols, and sometimes text or synthesized voice output. Because AAC relies on vision rather than sound/speech for access to the communication messages, it is critical to map out how children with DS examine and extract information from visual AAC displays. Otherwise there is the risk of implementing systems that are poorly matched to children’s skills and needs, a practice that in turn results in limited use or abandonment of the system. Few current AAC designs consider the fit between the system and the visual processing skills of users, and most are uninformed by empirical knowledge about human visual information processing. Moreover, little is known about visual processing in persons with significant communication limitations associated with DS. This research aims to improve the design of AAC displays through characterization of visual attention patterns to different AAC displays and their effects on functional use. Eye tracking - rarely used in DS - will reveal attention patterns/processes that typically go unrecorded in behavioral research. Our three-phase program will begin with eye tracking studies of visual attention under largely non-social laboratory conditions. In the next phase, we will introduce social interactions and record gaze path using mobile eye tracking technology. In the final phase, we will translate the knowledge gained in the laboratory studies to optimize functional communication in individuals with DS in performing tasks that represent typical daily life activities.</p>
<p><strong>Collaborators</strong></p>
<ul>
<li>Krista Wilkinson, Penn State, PI</li>
</ul>
<p><strong>Support</strong></p>
<p>The project is supported by NICHD under <a href="https://projectreporter.nih.gov/project_info_description.cfm?aid=9194421&amp;icde=0">R01HD083381-02</a>.</p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
